{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b341e-d762-4c93-ae88-c3a068313067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM getter\n",
    "from model import get_conversation\n",
    "# Veri seti oluşturucu LLM model (LLama-450B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5580b-4dd7-4fbc-8148-dda06f175a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tekli Link getter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        urls = [urljoin(url, link.get('href')) for link in links if link.get('href') is not None]\n",
    "        \n",
    "        return urls\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "url = 'https://www.webtekno.com/'\n",
    "url = 'https://www.webtekno.com/kripto-para'\n",
    "url = 'https://www.webtekno.com/iflas-eden-kripto-para-borsalari-h146330.html'\n",
    "url = 'https://finansalokuryazarlik.spl.com.tr/'\n",
    "url = 'https://www.parasut.com/blog/finansal-okuryazarlik-nedir-ve-nasil-gelistirilir'\n",
    "\n",
    "links = list(set(get_links(url)))\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163cf30-0c33-4771-bac8-9204260e0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "# Recursive text getter\n",
    "\n",
    "def extract_long_segments(text, threshold=2):\n",
    "    # Split the text into segments using \\n as the delimiter\n",
    "    segments = text.split('\\n')\n",
    "    \n",
    "    # Initialize an empty list to hold the valid segments\n",
    "    long_segments = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Split each segment into sentences\n",
    "        sentences = segment.split('.')\n",
    "        # Count the number of sentences with at least one word\n",
    "        sentence_count = sum(1 for sentence in sentences if sentence.strip())\n",
    "        \n",
    "        # If the number of sentences is greater than or equal to the threshold, add the segment to the result \n",
    "        if sentence_count >= threshold:\n",
    "            long_segments.append(segment)\n",
    "    \n",
    "    return long_segments\n",
    "    \n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a')\n",
    "        urls = [urljoin(url, link.get('href')) for link in links if link.get('href') is not None]\n",
    "        \n",
    "        return urls\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return ''\n",
    "\n",
    "def get_all_links(url, depth):\n",
    "    all_links = set()\n",
    "\n",
    "    def fetch_links(current_url, current_depth):\n",
    "        if current_depth > depth:\n",
    "            return\n",
    "        links = get_links(current_url)\n",
    "        for link in links:\n",
    "            if link not in all_links:\n",
    "                all_links.add(link)\n",
    "                fetch_links(link, current_depth + 1)\n",
    "\n",
    "    fetch_links(url, 1)\n",
    "    return all_links\n",
    "\n",
    "def get_all_texts(url, depth):\n",
    "    all_links = get_all_links(url, depth)\n",
    "    all_texts = [get_text(link) for link in all_links]\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size):\n",
    "    \"\"\"\n",
    "    Uzun bir metni kelimeleri bölmeden daha küçük parçalara böler.\n",
    "\n",
    "    ilgili parametreler:\n",
    "    \n",
    "    text (str): Bölünecek uzun metin.\n",
    "    chunk_size (int): Her bir parçanın maksimum boyutu.\n",
    "    Döndürür:\n",
    "    List[str]: Metin parçalarından oluşan bir liste.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        \n",
    "        if len(' '.join(current_chunk + [word])) > chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "url = 'https://finansalokuryazarlik.spl.com.tr/'\n",
    "#url = 'https://www.fo-der.org/finansal-okuryazarlik-nedir/'\n",
    "\n",
    "depth = 1\n",
    "all_texts = \"\".join(get_all_texts(url, depth))\n",
    "print(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01292698-0e0b-4842-ada7-a29a11143fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./finansalokuryazarlik\"\n",
    "os.makedirs(file_name+'_json/', exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a32d74-3135-42cc-aa9f-f8ece2686e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "web_scrape_command = \"\"\"\n",
    "    Verilen text'den Türkçe Dil modeli eğitim seti oluştur. Instruction1, Respnse1, Instruction2, Response2 .. .. formatında olsun ve Türkçe dilinde yaz.\n",
    "    Örnek bir çift:\n",
    "    Instruction1: Bir kişi, birikimlerini değerlendirmek için çeşitli yatırım araçları arasında hangisinin daha uygun olduğunu belirlemeye çalışıyor. Bu kişi, yüksek getiri potansiyeli ile birlikte risk seviyelerini de göz önünde bulundurarak nasıl bir yatırım stratejisi oluşturmalıdır?\n",
    "    \n",
    "    Response1: Risk ve Getiri Dengesini Anlamak:\n",
    "    Borsa: Yüksek getiri potansiyeline sahip olmakla birlikte, yüksek volatilite (dalgalanma) nedeniyle riski de yüksektir. \n",
    "    Devlet Tahvilleri: Daha düşük risk taşır ve sabit getiri sağlar. Özellikle riskten kaçınan ve sermayesini korumak isteyen yatırımcılar için uygundur. Ancak getirisi, borsa veya diğer riskli yatırımlara göre genellikle daha düşüktür.\n",
    "\n",
    "    \"\"\"\n",
    "conversation = get_conversation()\n",
    "\n",
    "chunk_size = 2000\n",
    "\n",
    "chunks = split_text_into_chunks(text, chunk_size)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    response = conversation.predict(human_input=web_scrape_command + chunk)\n",
    "    json_data = json.dumps(extract_data(response), ensure_ascii=False, indent=4)\n",
    "            \n",
    "    with open(file_name+ str(i)+\".json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a1e899a-f7e7-4ac1-bfe2-3b4eef9b328d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6da6b-92ef-438d-a279-6fd3e5968e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3bb38-085b-4756-b00f-41748254e9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d121b-32f5-4e52-a243-3c3cbef7f46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "okan",
   "language": "python",
   "name": "okan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b341e-d762-4c93-ae88-c3a068313067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM getter\n",
    "from model import get_conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5580b-4dd7-4fbc-8148-dda06f175a54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tekli Link getter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        urls = [urljoin(url, link.get('href')) for link in links if link.get('href') is not None]\n",
    "        \n",
    "        return urls\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "url = 'https://www.webtekno.com/'\n",
    "url = 'https://www.webtekno.com/kripto-para'\n",
    "url = 'https://www.webtekno.com/iflas-eden-kripto-para-borsalari-h146330.html'\n",
    "url = 'https://finansalokuryazarlik.spl.com.tr/'\n",
    "url = 'https://www.parasut.com/blog/finansal-okuryazarlik-nedir-ve-nasil-gelistirilir'\n",
    "\n",
    "links = list(set(get_links(url)))\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e25d8e-2a50-4eb4-91c5-7945a2fec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text getter\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract all text content from the webpage\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return ''\n",
    "\n",
    "url = 'https://www.webtekno.com/iflas-eden-kripto-para-borsalari-h146330.html'\n",
    "url = 'https://www.parasut.com/blog/finansal-okuryazarlik-nedir-ve-nasil-gelistirilir'\n",
    "url = 'https://www.turkiyefinansala.com/tr-tr/ala-hayat-blog/Sayfalar/finansal-okuryazarlik.aspx'\n",
    "text = get_text(url)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76b2ae-82df-4996-9c0b-a2f5f3a896c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_long_segments(text, threshold=2):\n",
    "    # Split the text into segments using \\n as the delimiter\n",
    "    segments = text.split('\\n')\n",
    "    \n",
    "    # Initialize an empty list to hold the valid segments\n",
    "    long_segments = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Split each segment into sentences\n",
    "        sentences = segment.split('.')\n",
    "        # Count the number of sentences with at least one word\n",
    "        sentence_count = sum(1 for sentence in sentences if sentence.strip())\n",
    "        \n",
    "        # If the number of sentences is greater than or equal to the threshold, add the segment to the result \n",
    "        if sentence_count >= threshold:\n",
    "            long_segments.append(segment)\n",
    "    \n",
    "    return long_segments\n",
    "\n",
    "#cumleler = extract_long_segments(text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a453b-cf1f-4101-8c92-3da8a2255550",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#lama3 chat !wget https://huggingface.co/bartowski/llama-3-neural-chat-v1-8b-GGUF/resolve/main/llama-3-neural-chat-v1-8b-Q8_0.gguf?download=true\n",
    "conversation = get_conversation()\n",
    "\n",
    "m = conversation.predict(human_input= \"merhaba\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf3770-6c95-40ee-9d1c-9f87abfb3974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recursive text getter\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a')\n",
    "        urls = [urljoin(url, link.get('href')) for link in links if link.get('href') is not None]\n",
    "        \n",
    "        return urls\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_all_links(url, depth):\n",
    "    all_links = set()\n",
    "\n",
    "    def fetch_links(current_url, current_depth):\n",
    "        if current_depth > depth:\n",
    "            return\n",
    "        links = get_links(current_url)\n",
    "        for link in links:\n",
    "            if link not in all_links:\n",
    "                all_links.add(link)\n",
    "                fetch_links(link, current_depth + 1)\n",
    "\n",
    "    fetch_links(url, 1)\n",
    "    return all_links\n",
    "\n",
    "url = 'https://www.turkiyefinansala.com/tr-tr/ala-hayat-blog/Sayfalar/finansal-okuryazarlik.aspx'\n",
    "depth = 1\n",
    "all_links = get_all_links(url, depth)\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86562c8-e9fe-41e1-b7ee-fc1d71ec3a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163cf30-0c33-4771-bac8-9204260e0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a')\n",
    "        urls = [urljoin(url, link.get('href')) for link in links if link.get('href') is not None]\n",
    "        \n",
    "        return urls\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return ''\n",
    "\n",
    "def get_all_links(url, depth):\n",
    "    all_links = set()\n",
    "\n",
    "    def fetch_links(current_url, current_depth):\n",
    "        if current_depth > depth:\n",
    "            return\n",
    "        links = get_links(current_url)\n",
    "        for link in links:\n",
    "            if link not in all_links:\n",
    "                all_links.add(link)\n",
    "                fetch_links(link, current_depth + 1)\n",
    "\n",
    "    fetch_links(url, 1)\n",
    "    return all_links\n",
    "\n",
    "def get_all_texts(url, depth):\n",
    "    all_links = get_all_links(url, depth)\n",
    "    all_texts = [get_text(link) for link in all_links]\n",
    "    return all_texts\n",
    "\n",
    "url = 'https://www.turkiyefinansala.com/tr-tr/ala-hayat-blog/Sayfalar/finansal-okuryazarlik.aspx'\n",
    "\n",
    "depth = 1\n",
    "all_texts = get_all_texts(url, depth)\n",
    "print(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39894b0d-3f16-42f2-9340-97274924c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_long_segments(all_texts[1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2ebfe-fa99-4957-8d48-983f67150c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01292698-0e0b-4842-ada7-a29a11143fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "okan",
   "language": "python",
   "name": "okan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
